"""
LLM Judge Utilities
===================
Helper functions for evaluating model outputs using an LLM as a judge.
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import os
import dotenv

# Load environment variables
dotenv.load_dotenv()

# Initialize Judge Model (using Nemotron or similar strong model for evaluation)
# We can reuse the keys from the main system
judge_llm = ChatOpenAI(
    model="nvidia/nemotron-nano-9b-v2:free",  # Using a capable model for judging
    temperature=0.1,  # Low temperature for consistent scoring
    openai_api_base="https://openrouter.ai/api/v1",
    openai_api_key=os.getenv("OPENROUTER_API_KEY3"),
    default_headers={
        "HTTP-Referer": "http://localhost:3000",
        "X-Title": "TrueSynth Judge"
    }
)

# Evaluation Prompt
evaluation_prompt = ChatPromptTemplate.from_template(
    """You are an impartial judge evaluating the quality of an AI-generated answer.
    
    You will be given:
    1. A Question
    2. A Predicted Answer (generated by the AI system)
    3. A Best Answer (ground truth)
    4. A list of Correct Answers (alternative ground truths)
    
    Your task is to evaluate the Predicted Answer based on two criteria:
    
    Criteria 1: Correctness (Score 0-10)
    - Does the predicted answer contain the core meaning of the ground truth?
    - **IMPORTANT**: Verbosity is allowed. If the answer is long but contains the correct fact, it should receive a high score.
    - Penalize only for missing facts or contradicting the ground truth.
    - 10 = Fully correct (even if verbose).
    - 0 = Completely incorrect.
    
    Criteria 2: Hallucination (Yes/No)
    - Does the predicted answer contain any false information not supported by the ground truth or general fact?
    - "No" means the answer is factual. "Yes" means it contains hallucinations.
    
    Output Format:
    Score: [0-10]
    Hallucination: [Yes/No]
    Reasoning: [Brief explanation]
    
    ---
    
    Question: {question}
    
    Best Answer: {best_answer}
    
    Correct Answers: {correct_answers}
    
    Predicted Answer: {predicted_answer}
    
    ---
    
    Evaluation:"""
)

judge_chain = (
    evaluation_prompt
    | judge_llm
    | StrOutputParser()
)

def evaluate_answer(question: str, predicted_answer: str, best_answer: str, correct_answers: str) -> dict:
    """
    Evaluate a single answer using the LLM judge.
    
    Returns:
        dict: {
            "score": int,
            "hallucination": bool,
            "reasoning": str
        }
    """
    try:
        result = judge_chain.invoke({
            "question": question,
            "predicted_answer": predicted_answer,
            "best_answer": best_answer,
            "correct_answers": correct_answers
        })
        
        # Parse the output using regex for robustness
        import re
        
        score = 0
        hallucination = False
        reasoning = ""
        
        # Parse Score
        score_match = re.search(r"Score:?\s*(\d+(?:\.\d+)?)", result, re.IGNORECASE)
        if score_match:
            try:
                score = float(score_match.group(1))
            except:
                score = 0
                
        # Parse Hallucination
        hallucination_match = re.search(r"Hallucination:?\s*(Yes|No)", result, re.IGNORECASE)
        if hallucination_match:
            hallucination = hallucination_match.group(1).lower() == "yes"
            
        # Parse Reasoning
        reasoning_match = re.search(r"Reasoning:?\s*(.*)", result, re.IGNORECASE | re.DOTALL)
        if reasoning_match:
            reasoning = reasoning_match.group(1).strip()
        else:
            # Fallback for reasoning if not explicitly labeled
            lines = result.strip().split('\n')
            if len(lines) > 2:
                reasoning = " ".join(lines[2:])
            
        return {
            "score": score,
            "hallucination": hallucination,
            "reasoning": reasoning
        }
        
    except Exception as e:
        print(f"Error in evaluation: {e}")
        return {
            "score": 0,
            "hallucination": True,
            "reasoning": f"Evaluation failed: {str(e)}"
        }

# ===========================
# TEXT METRICS UTILITIES
# ===========================
import re
import string
import collections

def normalize_answer(s):
    """Lower text and remove punctuation, articles and extra whitespace."""
    def remove_articles(text):
        regex = re.compile(r'\b(a|an|the)\b', re.UNICODE)
        return re.sub(regex, ' ', text)
    def white_space_fix(text):
        return ' '.join(text.split())
    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)
    def lower(text):
        return text.lower()
    return white_space_fix(remove_articles(remove_punc(lower(str(s)))))

def compute_soft_metrics(predicted_answer, ground_truth):
    """
    Compute Soft Metrics:
    - Soft Recall: Percentage of ground truth keywords present in prediction.
    - Soft Precision: Percentage of prediction keywords that are in ground truth.
    - Inclusion Score: 1.0 if all key ground truth tokens are covered, else 0.0.
    """
    pred_toks = normalize_answer(predicted_answer).split()
    gold_toks = normalize_answer(ground_truth).split()
    
    if not gold_toks:
        return 0.0, 0.0, 0.0

    if not pred_toks:
        return 0.0, 0.0, 0.0

    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)
    num_same = sum(common.values())
    
    # Soft Recall: How much of the ground truth did we cover?
    soft_recall = num_same / len(gold_toks)
    
    # Soft Precision: How much of our answer is "relevant"?
    # For verbose answers, this will naturally be lower, but we can check if key terms exist.
    soft_precision = num_same / len(pred_toks)
    
    # Inclusion Score: Did we get most of the key terms?
    inclusion_score = 1.0 if soft_recall >= 0.7 else 0.0 # Relaxed to 70% coverage
    
    return soft_recall, soft_precision, inclusion_score

def compute_text_metrics(predicted_answer, ground_truth):
    """
    Compute F1, Precision, and Recall between predicted answer and ground truth.
    Returns: (f1, precision, recall)
    """
    pred_toks = normalize_answer(predicted_answer).split()
    gold_toks = normalize_answer(ground_truth).split()
    
    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)
    num_same = sum(common.values())
    
    if len(gold_toks) == 0 or len(pred_toks) == 0:
        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise
        val = 1.0 if gold_toks == pred_toks else 0.0
        return val, val, val
    
    if num_same == 0:
        return 0.0, 0.0, 0.0
    
    precision = 1.0 * num_same / len(pred_toks)
    recall = 1.0 * num_same / len(gold_toks)
    f1 = (2 * precision * recall) / (precision + recall)
    
    return f1, precision, recall
